{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 教程&文章"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pip3 install scrapy\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.selector import Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://www.dianping.com/search/category/1/10/c3580o3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RestaurantSpider (scrapy.Spider):\n",
    "    name = 'restaurants'\n",
    "    \n",
    "    start_urls = [base_url + 'p' + str(page) for page in range(1,51)]\n",
    "    #     start_urls = [\n",
    "    #         'http://www.dianping.com/search/category/1/10/o3',\n",
    "    #     ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        selector = Selector(response)\n",
    "        div = selector.xpath('//div[@id=\"shop-all-list\"]/ul/li')\n",
    "        for dd in div:\n",
    "            title = dd.xpath('div[2]/div[1]/a[1]/h4/text()').extract_first()\n",
    "            image_url = dd.css('.pic img::attr(data-src)').extract_first()\n",
    "            shopurls = dd.xpath('div[2]/div[1]/a[1]/@href').extract()\n",
    "            url = 'http://www.dianping.com'+str(shopurls[0])\n",
    "\n",
    "            branch = dd.xpath('div[2]/div[1]/a[2]/text()').extract_first()\n",
    "\n",
    "            ratings_num = int(dd.xpath('div[2]/div[2]/a[1]/b/text()').extract_first())\n",
    "            price = dd.xpath('div[2]/div[2]/a[2]/b/text()').extract_first()\n",
    "\n",
    "            taste = float(dd.xpath('div[2]/span/span[1]/b/text()').extract_first())\n",
    "            environment = float(dd.xpath('div[2]/span/span[2]/b/text()').extract_first())\n",
    "            service = float(dd.xpath('div[2]/span/span[3]/b/text()').extract_first())\n",
    "\n",
    "            foodtype = dd.xpath('div[2]/div[3]/a[1]/span/text()').extract_first()\n",
    "            location = dd.xpath('div[2]/div[3]/a[2]/span/text()').extract_first()\n",
    "            address = dd.xpath('div[2]/div[3]/span[1]/text()').extract_first()\n",
    "\n",
    "            yield {\n",
    "                'title': title,\n",
    "                'image_url': image_url,\n",
    "                'url': url,\n",
    "                'ratings_num': ratings_num,\n",
    "                'branch': branch, \n",
    "                'price': price,\n",
    "                'taste': taste,\n",
    "                'environment': environment,\n",
    "                'service': service,\n",
    "                'type': foodtype,\n",
    "                'location': location,\n",
    "                'address': address\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-09 14:14:53 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: scrapybot)\n",
      "2018-02-09 14:14:53 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.3.1, w3lib 1.18.0, Twisted 17.9.0, Python 3.5.2 (default, Nov 23 2017, 16:37:01) - [GCC 5.4.0 20160609], pyOpenSSL 17.5.0 (OpenSSL 1.1.0g  2 Nov 2017), cryptography 2.1.4, Platform Linux-4.13.0-32-generic-x86_64-with-Ubuntu-16.04-xenial\n",
      "2018-02-09 14:14:53 [scrapy.crawler] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)', 'FEED_FORMAT': 'json', 'FEED_EXPORT_ENCODING': 'utf-8', 'FEED_URI': 'jiading.json', 'COOKIES_ENABLED': False, 'DOWNLOAD_DELAY': 3}\n",
      "2018-02-09 14:14:53 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2018-02-09 14:14:53 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2018-02-09 14:14:53 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2018-02-09 14:14:53 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2018-02-09 14:14:53 [scrapy.core.engine] INFO: Spider opened\n",
      "2018-02-09 14:14:53 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2018-02-09 14:14:53 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2018-02-09 14:14:53 [scrapy.core.engine] DEBUG: Crawled (403) <GET http://www.dianping.com/search/category/1/10/c3580o3p1> (referer: None)\n",
      "2018-02-09 14:14:54 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 http://www.dianping.com/search/category/1/10/c3580o3p1>: HTTP status code is not handled or not allowed\n",
      "2018-02-09 14:14:58 [scrapy.core.engine] DEBUG: Crawled (403) <GET http://www.dianping.com/search/category/1/10/c3580o3p2> (referer: None)\n",
      "2018-02-09 14:14:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 http://www.dianping.com/search/category/1/10/c3580o3p2>: HTTP status code is not handled or not allowed\n",
      "2018-02-09 14:15:02 [scrapy.core.engine] DEBUG: Crawled (403) <GET http://www.dianping.com/search/category/1/10/c3580o3p3> (referer: None)\n",
      "2018-02-09 14:15:02 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 http://www.dianping.com/search/category/1/10/c3580o3p3>: HTTP status code is not handled or not allowed\n",
      "2018-02-09 14:15:06 [scrapy.core.engine] DEBUG: Crawled (403) <GET http://www.dianping.com/search/category/1/10/c3580o3p4> (referer: None)\n",
      "2018-02-09 14:15:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 http://www.dianping.com/search/category/1/10/c3580o3p4>: HTTP status code is not handled or not allowed\n",
      "2018-02-09 14:15:09 [scrapy.core.engine] DEBUG: Crawled (403) <GET http://www.dianping.com/search/category/1/10/c3580o3p5> (referer: None)\n",
      "2018-02-09 14:15:09 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 http://www.dianping.com/search/category/1/10/c3580o3p5>: HTTP status code is not handled or not allowed\n",
      "2018-02-09 14:15:13 [scrapy.core.engine] DEBUG: Crawled (403) <GET http://www.dianping.com/search/category/1/10/c3580o3p6> (referer: None)\n",
      "2018-02-09 14:15:13 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 http://www.dianping.com/search/category/1/10/c3580o3p6>: HTTP status code is not handled or not allowed\n",
      "2018-02-09 14:15:17 [scrapy.core.engine] DEBUG: Crawled (403) <GET http://www.dianping.com/search/category/1/10/c3580o3p7> (referer: None)\n",
      "2018-02-09 14:15:17 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 http://www.dianping.com/search/category/1/10/c3580o3p7>: HTTP status code is not handled or not allowed\n",
      "2018-02-09 14:15:21 [scrapy.core.engine] DEBUG: Crawled (403) <GET http://www.dianping.com/search/category/1/10/c3580o3p8> (referer: None)\n",
      "2018-02-09 14:15:22 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 http://www.dianping.com/search/category/1/10/c3580o3p8>: HTTP status code is not handled or not allowed\n",
      "2018-02-09 14:15:25 [scrapy.core.engine] DEBUG: Crawled (403) <GET http://www.dianping.com/search/category/1/10/c3580o3p9> (referer: None)\n",
      "2018-02-09 14:15:25 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 http://www.dianping.com/search/category/1/10/c3580o3p9>: HTTP status code is not handled or not allowed\n",
      "2018-02-09 14:15:29 [scrapy.core.engine] DEBUG: Crawled (403) <GET http://www.dianping.com/search/category/1/10/c3580o3p10> (referer: None)\n",
      "2018-02-09 14:15:29 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 http://www.dianping.com/search/category/1/10/c3580o3p10>: HTTP status code is not handled or not allowed\n",
      "2018-02-09 14:15:33 [scrapy.core.engine] DEBUG: Crawled (403) <GET http://www.dianping.com/search/category/1/10/c3580o3p11> (referer: None)\n",
      "2018-02-09 14:15:33 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 http://www.dianping.com/search/category/1/10/c3580o3p11>: HTTP status code is not handled or not allowed\n",
      "2018-02-09 14:15:37 [scrapy.core.engine] DEBUG: Crawled (403) <GET http://www.dianping.com/search/category/1/10/c3580o3p12> (referer: None)\n",
      "2018-02-09 14:15:38 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 http://www.dianping.com/search/category/1/10/c3580o3p12>: HTTP status code is not handled or not allowed\n",
      "2018-02-09 14:15:42 [scrapy.core.engine] DEBUG: Crawled (403) <GET http://www.dianping.com/search/category/1/10/c3580o3p13> (referer: None)\n",
      "2018-02-09 14:15:42 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 http://www.dianping.com/search/category/1/10/c3580o3p13>: HTTP status code is not handled or not allowed\n",
      "2018-02-09 14:15:46 [scrapy.core.engine] DEBUG: Crawled (403) <GET http://www.dianping.com/search/category/1/10/c3580o3p14> (referer: None)\n",
      "2018-02-09 14:15:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 http://www.dianping.com/search/category/1/10/c3580o3p14>: HTTP status code is not handled or not allowed\n",
      "2018-02-09 14:15:50 [scrapy.core.engine] DEBUG: Crawled (403) <GET http://www.dianping.com/search/category/1/10/c3580o3p15> (referer: None)\n",
      "2018-02-09 14:15:50 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 http://www.dianping.com/search/category/1/10/c3580o3p15>: HTTP status code is not handled or not allowed\n",
      "2018-02-09 14:15:53 [scrapy.extensions.logstats] INFO: Crawled 15 pages (at 15 pages/min), scraped 0 items (at 0 items/min)\n",
      "2018-02-09 14:15:55 [scrapy.core.engine] DEBUG: Crawled (403) <GET http://www.dianping.com/search/category/1/10/c3580o3p16> (referer: None)\n",
      "2018-02-09 14:15:55 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 http://www.dianping.com/search/category/1/10/c3580o3p16>: HTTP status code is not handled or not allowed\n",
      "2018-02-09 14:15:58 [scrapy.core.engine] DEBUG: Crawled (403) <GET http://www.dianping.com/search/category/1/10/c3580o3p17> (referer: None)\n",
      "2018-02-09 14:15:58 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 http://www.dianping.com/search/category/1/10/c3580o3p17>: HTTP status code is not handled or not allowed\n",
      "2018-02-09 14:16:03 [scrapy.core.engine] DEBUG: Crawled (403) <GET http://www.dianping.com/search/category/1/10/c3580o3p18> (referer: None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-09 14:16:03 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 http://www.dianping.com/search/category/1/10/c3580o3p18>: HTTP status code is not handled or not allowed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'jiading.json',\n",
    "    'FEED_EXPORT_ENCODING':'utf-8',\n",
    "    'COOKIES_ENABLED': False,\n",
    "    'DOWNLOAD_DELAY': 3,\n",
    "})\n",
    "\n",
    "process.crawl(RestaurantSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
